import streamlit as st
import requests
from bs4 import BeautifulSoup
from notion_client import Client
from urllib.parse import urlparse, urljoin
from datetime import datetime
import time
import re
import json
import base64
from io import BytesIO

# å›ºå®šã®èªè¨¼æƒ…å ±
NOTION_API_TOKEN = "ntn_i2957150244j9hSJCmlhWx1tkxlBP2MNliQk9Z3AkBHgcK"  # ã‚ãªãŸã®å®Ÿéš›ã®APIãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã¦ãã ã•ã„
DATABASE_ID = "1b90b0428824814fa0d9db921aa812d0"  # ã‚ãªãŸã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹IDã«ç½®ãæ›ãˆã¦ãã ã•ã„

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.set_page_config(
    page_title="Notion Bookmarker",
    page_icon="ğŸ“š",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ã‚«ã‚¹ã‚¿ãƒ CSSã‚’é©ç”¨ï¼ˆçœç•¥ï¼‰
st.markdown("""<style>/* ã‚¹ã‚¿ã‚¤ãƒ«çœç•¥ */</style>""", unsafe_allow_html=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'page_info' not in st.session_state:
    st.session_state['page_info'] = None
if 'loading' not in st.session_state:
    st.session_state['loading'] = False
if 'saving' not in st.session_state:
    st.session_state['saving'] = False
if 'success' not in st.session_state:
    st.session_state['success'] = False
if 'error' not in st.session_state:
    st.session_state['error'] = None
if 'notion_url' not in st.session_state:
    st.session_state['notion_url'] = None
if 'raw_html' not in st.session_state:
    st.session_state['raw_html'] = None

# ãƒ¡ã‚¤ãƒ³ç”»é¢è¡¨ç¤º - ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
st.markdown("<h1>Notion Bookmarker</h1>", unsafe_allow_html=True)

# è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
USER_AGENTS = [
    # ãƒ¢ãƒã‚¤ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (iOS)
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
    # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Chrome)
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Firefox)
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0',
    # ãƒ¢ãƒã‚¤ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Android)
    'Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.104 Mobile Safari/537.36'
]

# ç”Ÿã®HTMLã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def display_raw_html(html):
    """HTMLå†…å®¹ã‚’åˆ†æã—ã€é‡è¦ãªéƒ¨åˆ†ã‚’è¡¨ç¤ºã™ã‚‹"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # é‡è¦ãªéƒ¨åˆ†ã‚’æŠ½å‡º
    head_content = soup.head.prettify() if soup.head else "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    
    # ã‚¿ã‚¤ãƒˆãƒ«é–¢é€£ã®è¦ç´ 
    title_tag = soup.title.prettify() if soup.title else "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    og_tags = [str(tag) for tag in soup.find_all('meta', property=re.compile(r'^og:'))]
    twitter_tags = [str(tag) for tag in soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')})]
    
    # h1ã‚¿ã‚°
    h1_tags = [str(tag) for tag in soup.find_all('h1')]
    
    # ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤è¦ç´  (iwaraã‚µã‚¤ãƒˆå‘ã‘)
    title_classes = []
    for cls in ['video-title', 'title', 'heading', 'header-title']:
        elements = soup.find_all(class_=re.compile(cls, re.I))
        for el in elements:
            title_classes.append(f"Class '{cls}': {str(el)}")
    
    # è¡¨ç¤ºç”¨ã®Markdown
    st.markdown("### HTMLã®é‡è¦éƒ¨åˆ†")
    
    with st.expander("titleã‚¿ã‚°", expanded=False):
        st.code(title_tag, language="html")
        
    with st.expander("OGPãƒ¡ã‚¿ã‚¿ã‚°", expanded=False):
        if og_tags:
            for tag in og_tags:
                st.code(tag, language="html")
        else:
            st.write("OGPãƒ¡ã‚¿ã‚¿ã‚°ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    with st.expander("Twitterã‚«ãƒ¼ãƒ‰ãƒ¡ã‚¿ã‚¿ã‚°", expanded=False):
        if twitter_tags:
            for tag in twitter_tags:
                st.code(tag, language="html")
        else:
            st.write("Twitterã‚«ãƒ¼ãƒ‰ãƒ¡ã‚¿ã‚¿ã‚°ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    with st.expander("h1ã‚¿ã‚°", expanded=False):
        if h1_tags:
            for tag in h1_tags:
                st.code(tag, language="html")
        else:
            st.write("h1ã‚¿ã‚°ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    with st.expander("ã‚¿ã‚¤ãƒˆãƒ«é–¢é€£ã®ã‚¯ãƒ©ã‚¹", expanded=False):
        if title_classes:
            for cls in title_classes:
                st.code(cls, language="html")
        else:
            st.write("ã‚¿ã‚¤ãƒˆãƒ«é–¢é€£ã®ã‚¯ãƒ©ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆé«˜åº¦åŒ–ç‰ˆ
def get_metadata_advanced(url):
    """é«˜åº¦ãªæ–¹æ³•ã§Webãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    
    # åˆæœŸåŒ–
    page_info = {
        'title': None,
        'description': None,
        'thumbnail': None,
        'url': url,
        'domain': urlparse(url).netloc
    }
    raw_html = None
    best_html = None
    debug_info = {}
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦å†åˆ©ç”¨ã™ã‚‹
    session = requests.Session()
    
    # è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ˜ãƒƒãƒ€ãƒ¼ã§è©¦ã™
    for idx, agent in enumerate(USER_AGENTS):
        try:
            headers = {
                'User-Agent': agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Referer': 'https://www.google.com/',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'dnt': '1'
            }
            
            # urlãŒiwara.tvã®å ´åˆã€ç‰¹åˆ¥ãªå‡¦ç†
            if 'iwara' in url:
                headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                headers['Accept-Language'] = 'ja,en-US;q=0.7,en;q=0.3'
                headers['Referer'] = 'https://www.iwara.tv/'
            
            debug_info[f'request_{idx}'] = {
                'user_agent': agent,
                'headers': headers
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = session.get(url, headers=headers, timeout=20, allow_redirects=True)
            debug_info[f'response_{idx}'] = {
                'status_code': response.status_code,
                'content_type': response.headers.get('Content-Type', 'unknown'),
                'encoding': response.encoding
            }
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ200ä»¥å¤–ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if response.status_code != 200:
                continue
            
            # HTMLè§£æ
            html_content = response.text
            
            # æœ€åˆã®æˆåŠŸã—ãŸHTMLã‚’ä¿å­˜
            if raw_html is None:
                raw_html = html_content
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„HTMLã‚’æ¤œç´¢
            if 'title' in html_content.lower() or 'og:title' in html_content.lower():
                best_html = html_content
                break
        
        except Exception as e:
            debug_info[f'error_{idx}'] = str(e)
            continue
    
    # æœ€è‰¯ã®HTMLã‚’ä½¿ç”¨ï¼ˆãªã‘ã‚Œã°æœ€åˆã®HTMLï¼‰
    html_to_parse = best_html or raw_html
    
    # HTMLè§£æ
    if html_to_parse:
        try:
            soup = BeautifulSoup(html_to_parse, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_candidates = []
            
            # 1. Open Graph Title
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                title_candidates.append(('og:title', og_title.get('content').strip()))
            
            # 2. Twitter Card Title
            twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
            if twitter_title and twitter_title.get('content'):
                title_candidates.append(('twitter:title', twitter_title.get('content').strip()))
            
            # 3. HTML Title
            if soup.title and soup.title.string:
                title_text = soup.title.string.strip()
                # ã‚µã‚¤ãƒˆåã‚’é™¤å»ã™ã‚‹å‡¦ç†
                title_text = re.sub(r'\s*[|\-â€“â€”]\s*.*$', '', title_text)
                title_candidates.append(('html_title', title_text))
            
            # 4. H1 Tag
            h1 = soup.find('h1')
            if h1 and h1.text.strip():
                title_candidates.append(('h1', h1.text.strip()))
            
            # 5. ç‰¹å®šã®ã‚µã‚¤ãƒˆå‘ã‘ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†
            if 'iwara' in url:
                # video-titleã‚¯ãƒ©ã‚¹ã‚’æ¢ã™
                video_title = soup.find(class_='video-title')
                if video_title and video_title.text.strip():
                    title_candidates.append(('iwara_video_title', video_title.text.strip()))
                
                # nodeã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
                node_title = soup.find(class_='node-title')
                if node_title and node_title.text.strip():
                    title_candidates.append(('iwara_node_title', node_title.text.strip()))
                
                # h4ã‚¿ã‚°ã‚’æ¢ã™ (iwaraã®ä¸€éƒ¨ãƒšãƒ¼ã‚¸ã§ä½¿ç”¨)
                h4_title = soup.find('h4')
                if h4_title and h4_title.text.strip():
                    title_candidates.append(('iwara_h4', h4_title.text.strip()))
            
            # æœ€è‰¯ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
            if title_candidates:
                # ã‚¿ã‚¤ãƒˆãƒ«å€™è£œã‚’è¨˜éŒ²
                debug_info['title_candidates'] = title_candidates
                
                # æœ€åˆã®å€™è£œã‚’ä½¿ç”¨ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
                page_info['title'] = title_candidates[0][1]
            else:
                page_info['title'] = f"Saved from {page_info['domain']}"
            
            # èª¬æ˜æ–‡æŠ½å‡º
            description_candidates = []
            
            # 1. Open Graph Description
            og_desc = soup.find('meta', property='og:description')
            if og_desc and og_desc.get('content'):
                description_candidates.append(('og:description', og_desc.get('content').strip()))
            
            # 2. Twitter Card Description
            twitter_desc = soup.find('meta', attrs={'name': 'twitter:description'})
            if twitter_desc and twitter_desc.get('content'):
                description_candidates.append(('twitter:description', twitter_desc.get('content').strip()))
            
            # 3. Meta Description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                description_candidates.append(('meta_description', meta_desc.get('content').strip()))
            
            # æœ€è‰¯ã®èª¬æ˜æ–‡ã‚’é¸æŠ
            if description_candidates:
                page_info['description'] = description_candidates[0][1]
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒæŠ½å‡º
            image_candidates = []
            
            # 1. Open Graph Image
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                image_url = og_image.get('content').strip()
                if not image_url.startswith(('http://', 'https://')):
                    image_url = urljoin(url, image_url)
                image_candidates.append(('og:image', image_url))
            
            # 2. Twitter Card Image
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                image_url = twitter_image.get('content').strip()
                if not image_url.startswith(('http://', 'https://')):
                    image_url = urljoin(url, image_url)
                image_candidates.append(('twitter:image', image_url))
            
            # 3. ç‰¹å®šã®ã‚µã‚¤ãƒˆå‘ã‘ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†
            if 'iwara' in url:
                # ãƒ“ãƒ‡ã‚ªã‚µãƒ ãƒã‚¤ãƒ«
                video_thumb = soup.find('img', class_='video-thumbnail')
                if video_thumb and video_thumb.get('src'):
                    image_url = video_thumb.get('src').strip()
                    if not image_url.startswith(('http://', 'https://')):
                        image_url = urljoin(url, image_url)
                    image_candidates.append(('iwara_video_thumbnail', image_url))
            
            # æœ€è‰¯ã®ç”»åƒã‚’é¸æŠ
            if image_candidates:
                page_info['thumbnail'] = image_candidates[0][1]
        
        except Exception as e:
            debug_info['parsing_error'] = str(e)
    
    # ã‚¿ã‚¤ãƒˆãƒ«ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not page_info['title'] or page_info['title'] == f"Saved from {page_info['domain']}":
        # MetaScraperã‚µãƒ¼ãƒ“ã‚¹ã‚’è©¦ã™
        try:
            meta_response = requests.get(f"https://api.microlink.io/?url={url}")
            if meta_response.status_code == 200:
                meta_data = meta_response.json()
                if meta_data.get('status') == 'success':
                    data = meta_data.get('data', {})
                    if data.get('title'):
                        page_info['title'] = data['title']
                    if data.get('description') and not page_info['description']:
                        page_info['description'] = data['description']
                    if data.get('image') and data['image'].get('url') and not page_info['thumbnail']:
                        page_info['thumbnail'] = data['image']['url']
        except:
            pass
    
    # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not page_info['title'] or page_info['title'] == f"Saved from {page_info['domain']}":
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰ã®ç”Ÿæˆ
        domain_parts = page_info['domain'].split('.')
        if len(domain_parts) > 1:
            page_info['title'] = f"Content from {domain_parts[-2].capitalize()}"
    
    # çµæœã‚’è¿”ã™
    return page_info, raw_html, debug_info

# Notionã«æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°
def add_to_notion(page_info):
    try:
        # Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        notion = Client(auth=NOTION_API_TOKEN)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        db = notion.databases.retrieve(database_id=DATABASE_ID)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ç¢ºèª
        properties = {}
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (å¿…é ˆ) ã‚’æ¤œå‡º
        title_field = None
        for name, prop in db['properties'].items():
            if prop['type'] == 'title':
                title_field = name
                properties[name] = {
                    'title': [{'text': {'content': page_info['title']}}]
                }
                break
        
        if not title_field:
            return False, "ã‚¿ã‚¤ãƒˆãƒ«ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        # URLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'URL' in db['properties'] and db['properties']['URL']['type'] == 'url':
            properties['URL'] = {'url': page_info['url']}
        
        # ã‚¿ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ã‚¿ã‚°' in db['properties'] and db['properties']['ã‚¿ã‚°']['type'] == 'multi_select':
            properties['ã‚¿ã‚°'] = {'multi_select': []}
        
        # ä½œæˆæ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ä½œæˆæ—¥æ™‚' in db['properties'] and db['properties']['ä½œæˆæ—¥æ™‚']['type'] == 'date':
            properties['ä½œæˆæ—¥æ™‚'] = {
                'date': {
                    'start': datetime.now().isoformat()
                }
            }
        
        # Notionãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
        new_page = notion.pages.create(
            parent={'database_id': DATABASE_ID},
            properties=properties
        )
        
        # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€å­ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ 
        if page_info['thumbnail']:
            try:
                notion.blocks.children.append(
                    block_id=new_page['id'],
                    children=[
                        {
                            "object": "block",
                            "type": "image",
                            "image": {
                                "type": "external",
                                "external": {
                                    "url": page_info['thumbnail']
                                }
                            }
                        }
                    ]
                )
            except Exception:
                pass
        
        return True, new_page['url']
    
    except Exception as e:
        return False, str(e)

# URLå…¥åŠ›ã‚¨ãƒªã‚¢ - ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥ãªãƒ‡ã‚¶ã‚¤ãƒ³

# URLã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
url = st.text_input("", placeholder="https://example.com", label_visibility="collapsed")

# æ¤œç´¢ãƒœã‚¿ãƒ³ã¨ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã®ç®¡ç†
col1, col2 = st.columns([1, 3])
with col1:
    fetch_button = st.button("æƒ…å ±ã‚’å–å¾—", key="fetch_button", use_container_width=True)
with col2:
    pass

if fetch_button and url:
    st.session_state['loading'] = True
    st.session_state['page_info'] = None
    st.session_state['success'] = False
    st.session_state['error'] = None
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = st.progress(0)
    for percent_complete in range(0, 101, 10):
        time.sleep(0.05)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®é…å»¶
        progress_bar.progress(percent_complete)
    
    # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    page_info, raw_html, debug_info = get_metadata_advanced(url)
    st.session_state['page_info'] = page_info
    st.session_state['raw_html'] = raw_html
    st.session_state['debug_info'] = debug_info
    st.session_state['loading'] = False
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®Œäº†çŠ¶æ…‹ã«ã—ã¦å°‘ã—å¾…ã£ã¦ã‹ã‚‰æ¶ˆã™
    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()
    
    # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º
    st.rerun()

# æƒ…å ±ã‚’å–å¾—ã—ãŸå¾Œã®è¡¨ç¤º
if st.session_state['page_info']:
    page_info = st.session_state['page_info']
    
    # æŠ½å‡ºã—ãŸæƒ…å ±ã‚’ã‚«ãƒ¼ãƒ‰UIã§è¡¨ç¤º
    st.markdown("""
    <h2>å–å¾—ã—ãŸæƒ…å ±</h2>
    """, unsafe_allow_html=True)
    
    st.markdown(f"""
    <div class="info-card">
        <h3 style="margin-top: 0; margin-bottom: 0.75rem; font-size: 1.4rem;">{page_info['title']}</h3>
        <a href="{page_info['url']}" target="_blank" style="color: #4361EE; text-decoration: none; font-size: 1rem; display: block; margin-bottom: 0.75rem;">{page_info['url']}</a>
        <div class="domain-badge">{page_info['domain']}</div>
    </div>
    """, unsafe_allow_html=True)
    
    # ã‚µãƒ ãƒã‚¤ãƒ« - éæ¨å¥¨ã®use_column_widthã‚’use_container_widthã«å¤‰æ›´
    if page_info['thumbnail']:
        st.image(page_info['thumbnail'], caption="ã‚µãƒ ãƒã‚¤ãƒ«", use_container_width=True)
    
    # èª¬æ˜
    if page_info.get('description'):
        st.markdown("**èª¬æ˜**:")
        st.write(page_info['description'])
    
    # ã‚¿ã‚¤ãƒˆãƒ«æ‰‹å‹•ç·¨é›†æ©Ÿèƒ½
    st.subheader("ã‚¿ã‚¤ãƒˆãƒ«ã®ç·¨é›†")
    edited_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç·¨é›†:", value=page_info['title'])
    if edited_title != page_info['title']:
        page_info['title'] = edited_title
        st.session_state['page_info'] = page_info
        st.success("ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    
    # ä¿å­˜ãƒœã‚¿ãƒ³ - ã‚¢ã‚¯ã‚»ãƒ³ãƒˆã‚«ãƒ©ãƒ¼ä½¿ç”¨
    save_col1, save_col2 = st.columns([1, 3])
    with save_col1:
        save_button = st.button("Notionã«ä¿å­˜", key="save_button", use_container_width=True)
    with save_col2:
        pass
    
    # ä¿å­˜ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã¨ã
    if save_button:
        st.session_state['saving'] = True
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
        save_progress = st.progress(0)
        for percent_complete in range(0, 101, 20):
            time.sleep(0.1)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®é…å»¶
            save_progress.progress(percent_complete)
        
        # Notionã«ä¿å­˜
        success, result = add_to_notion(page_info)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®Œäº†çŠ¶æ…‹ã«ã—ã¦å°‘ã—å¾…ã£ã¦ã‹ã‚‰æ¶ˆã™
        save_progress.progress(100)
        time.sleep(0.5)
        save_progress.empty()
        
        st.session_state['saving'] = False
        st.session_state['success'] = success
        
        if success:
            st.session_state['notion_url'] = result
        else:
            st.session_state['error'] = result
        
        # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦çµæœã‚’è¡¨ç¤º
        st.rerun()
    
    # ä¿å­˜æˆåŠŸæ™‚ã®è¡¨ç¤º
    if st.session_state['success']:
        st.success("âœ… Notionã«ä¿å­˜ã—ã¾ã—ãŸï¼")
        
        if st.session_state.get('notion_url'):
            st.markdown(f"[Notionã§é–‹ã]({st.session_state['notion_url']})")
    
    # ã‚¨ãƒ©ãƒ¼æ™‚ã®è¡¨ç¤º
    if st.session_state['error']:
        st.error(f"âŒ ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {st.session_state['error']}")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("""
<footer>
    <p>Â© 2025 Notion Bookmarker - ã™ã¹ã¦ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’Notionã«ä¿å­˜</p>
</footer>
""", unsafe_allow_html=True)
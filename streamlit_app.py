import streamlit as st
import requests
from bs4 import BeautifulSoup
from notion_client import Client
from urllib.parse import urlparse, urljoin
from datetime import datetime
import time
import re
import json
import random
import http.cookiejar
import base64
from io import BytesIO
from PIL import Image

# å›ºå®šã®èªè¨¼æƒ…å ±
NOTION_API_TOKEN = "ntn_i2957150244j9hSJCmlhWx1tkxlBP2MNliQk9Z3AkBHgcK"  # ã‚ãªãŸã®å®Ÿéš›ã®APIãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã¦ãã ã•ã„
DATABASE_ID = "1b90b0428824814fa0d9db921aa812d0"  # ã‚ãªãŸã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹IDã«ç½®ãæ›ãˆã¦ãã ã•ã„

# æˆäººå‘ã‘ã‚µã‚¤ãƒˆå¯¾å¿œã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
class AdultSiteScraper:
    def __init__(self):
        # ã‚¯ãƒƒã‚­ãƒ¼ã‚’ä¿å­˜ã™ã‚‹ã‚¯ãƒƒã‚­ãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
        self.cookie_jar = http.cookiejar.CookieJar()
        
        # æ§˜ã€…ãªUser-Agentã‚’ç”¨æ„
        self.user_agents = [
            # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ–ãƒ©ã‚¦ã‚¶
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
            # ãƒ¢ãƒã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶
            'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.104 Mobile Safari/537.36'
        ]
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦å†åˆ©ç”¨ã™ã‚‹
        self.session = requests.Session()
        self.session.cookies = self.cookie_jar
    
    def get_page_info(self, url):
        """Webãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
        # åˆæœŸåŒ–
        page_info = {
            'title': None,
            'description': None,
            'thumbnail': None,
            'url': url,
            'domain': urlparse(url).netloc
        }
        raw_html = None
        debug_info = {}
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®å‡¦ç†ã‚’é©ç”¨
        domain = urlparse(url).netloc
        
        # ç‰¹æ®Šãªã‚µã‚¤ãƒˆã®å‡¦ç†
        if 'japaneseasmr.com' in domain:
            return self._process_japaneseasmr(url, page_info)
        elif 'supjav.com' in domain:
            return self._process_supjav(url, page_info)
        elif 'iwara' in domain:
            return self._process_iwara(url, page_info)
        else:
            # ä¸€èˆ¬çš„ãªã‚µã‚¤ãƒˆã®å‡¦ç†
            return self._process_general_site(url, page_info)
    
    def _process_japaneseasmr(self, url, page_info):
        """japaneseasmr.comã®ãƒšãƒ¼ã‚¸å‡¦ç†"""
        debug_info = {}
        
        try:
            # æˆäººå‘ã‘ã‚µã‚¤ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Referer': 'https://www.google.com/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Pragma': 'no-cache',
                'Cache-Control': 'no-cache'
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ 
            response = self.session.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                raw_html = response.text
                soup = BeautifulSoup(raw_html, 'html.parser')
                
                # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                if soup.title and soup.title.string and 'just a moment' not in soup.title.string.lower():
                    page_info['title'] = soup.title.string.strip()
                else:
                    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
                    article_title = soup.find('h1', class_='article-title')
                    if article_title:
                        page_info['title'] = article_title.text.strip()
                
                # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’æŠ½å‡º - japaneseasmrç‰¹æœ‰ã®å‡¦ç†
                # æ–¹æ³•1: ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒ
                thumbnail = soup.find('div', class_='eye-catch')
                if thumbnail and thumbnail.find('img'):
                    img = thumbnail.find('img')
                    if img.get('src'):
                        page_info['thumbnail'] = urljoin(url, img.get('src'))
                
                # æ–¹æ³•2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®æœ€åˆã®ç”»åƒ
                if not page_info['thumbnail']:
                    article_content = soup.find('div', class_='article-body')
                    if article_content:
                        img_tags = article_content.find_all('img')
                        for img in img_tags:
                            if img.get('src') and not img.get('src').endswith(('.gif', 'spacer.png', 'blank.gif')):
                                page_info['thumbnail'] = urljoin(url, img.get('src'))
                                break
                
                # æ–¹æ³•3: imgè¦ç´ ã®data-srcå±æ€§
                if not page_info['thumbnail']:
                    for img in soup.find_all('img', attrs={'data-src': True}):
                        if not img.get('data-src').endswith(('.gif', 'spacer.png', 'blank.gif')):
                            page_info['thumbnail'] = urljoin(url, img.get('data-src'))
                            break
                
                # æ–¹æ³•4: articleã‚¿ã‚°ã®èƒŒæ™¯ç”»åƒ
                if not page_info['thumbnail']:
                    article = soup.find('article')
                    if article:
                        style = article.get('style')
                        if style and 'background-image' in style:
                            # æ­£è¦è¡¨ç¾ã§URLæŠ½å‡º
                            bg_match = re.search(r'background-image:\s*url\([\'"]?(.*?)[\'"]?\)', style)
                            if bg_match:
                                page_info['thumbnail'] = urljoin(url, bg_match.group(1))
                
                # èª¬æ˜æŠ½å‡º
                meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', property='og:description')
                if meta_desc and meta_desc.get('content'):
                    page_info['description'] = meta_desc.get('content').strip()
                else:
                    # è¨˜äº‹ã®æœ€åˆã®æ®µè½
                    first_para = soup.find('div', class_='article-body').find('p')
                    if first_para:
                        page_info['description'] = first_para.text.strip()[:200]
            
            return page_info, raw_html, debug_info
            
        except Exception as e:
            debug_info['error'] = str(e)
            return page_info, None, debug_info
    
    def _process_supjav(self, url, page_info):
        """supjav.comã®ãƒšãƒ¼ã‚¸å‡¦ç†"""
        debug_info = {}
        
        try:
            # æˆäººå‘ã‘ã‚µã‚¤ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Referer': 'https://www.google.com/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cookie': 'kt_tcookie=1; kt_is_visited=1; kt_ips=127.0.0.1',  # æˆäººå‘ã‘ã‚µã‚¤ãƒˆã§å¿…è¦ãªã“ã¨ãŒã‚ã‚‹
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = self.session.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                raw_html = response.text
                soup = BeautifulSoup(raw_html, 'html.parser')
                
                # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                if soup.title and soup.title.string and 'just a moment' not in soup.title.string.lower():
                    page_info['title'] = soup.title.string.strip()
                else:
                    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ ã‚’æ¢ã™
                    article_title = soup.find('h1', class_='article-title') or soup.find('h1', class_='entry-title')
                    if article_title:
                        page_info['title'] = article_title.text.strip()
                
                # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’æŠ½å‡º - supjavç‰¹æœ‰ã®å‡¦ç†
                # æ–¹æ³•1: ã‚µãƒ ãƒã‚¤ãƒ«å°‚ç”¨ã‚¯ãƒ©ã‚¹
                for class_name in ['thumb', 'thumbnail', 'wp-post-image', 'video-thumb']:
                    if not page_info['thumbnail']:
                        thumbnail = soup.find('img', class_=class_name)
                        if thumbnail and thumbnail.get('src'):
                            page_info['thumbnail'] = urljoin(url, thumbnail.get('src'))
                
                # æ–¹æ³•2: ãƒ“ãƒ‡ã‚ªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒ
                if not page_info['thumbnail']:
                    video_preview = soup.find('div', class_='video-preview')
                    if video_preview and video_preview.find('img'):
                        img = video_preview.find('img')
                        if img.get('src'):
                            page_info['thumbnail'] = urljoin(url, img.get('src'))
                
                # æ–¹æ³•3: ãƒ‡ãƒ¼ã‚¿å±æ€§ã®ã‚ã‚‹ç”»åƒ
                if not page_info['thumbnail']:
                    for attr in ['data-src', 'data-lazy-src', 'data-original']:
                        for img in soup.find_all('img', attrs={attr: True}):
                            if not img.get(attr).endswith(('.gif', 'spacer.png')):
                                page_info['thumbnail'] = urljoin(url, img.get(attr))
                                break
                        if page_info['thumbnail']:
                            break
                
                # æ–¹æ³•4: ã‚¹ã‚¿ã‚¤ãƒ«å±æ€§ã‹ã‚‰èƒŒæ™¯ç”»åƒã‚’æŠ½å‡º
                if not page_info['thumbnail']:
                    elements_with_style = soup.find_all(style=re.compile(r'background(-image)?:\s*url'))
                    for element in elements_with_style:
                        style = element.get('style')
                        bg_match = re.search(r'background(-image)?:\s*url\([\'"]?(.*?)[\'"]?\)', style)
                        if bg_match:
                            page_info['thumbnail'] = urljoin(url, bg_match.group(2))
                            break
                
                # æ–¹æ³•5: ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰ã®æŠ½å‡º
                if not page_info['thumbnail']:
                    og_image = soup.find('meta', property='og:image')
                    if og_image and og_image.get('content'):
                        page_info['thumbnail'] = og_image.get('content')
                
                # èª¬æ˜æŠ½å‡º
                meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', property='og:description')
                if meta_desc and meta_desc.get('content'):
                    page_info['description'] = meta_desc.get('content').strip()
            
            return page_info, raw_html, debug_info
            
        except Exception as e:
            debug_info['error'] = str(e)
            return page_info, None, debug_info
    
    def _process_iwara(self, url, page_info):
        """iwaraã‚µã‚¤ãƒˆã®å‡¦ç†"""
        debug_info = {}
        
        try:
            # iwaraã‚µã‚¤ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Referer': 'https://www.iwara.tv/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = self.session.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                raw_html = response.text
                soup = BeautifulSoup(raw_html, 'html.parser')
                
                # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                title_candidates = []
                
                # 1. video-titleã‚¯ãƒ©ã‚¹ã‚’æ¢ã™
                video_title = soup.find(class_='video-title')
                if video_title and video_title.text.strip():
                    title_candidates.append(video_title.text.strip())
                
                # 2. nodeã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
                node_title = soup.find(class_='node-title')
                if node_title and node_title.text.strip():
                    title_candidates.append(node_title.text.strip())
                
                # 3. h1ã‚¿ã‚°ã‚’æ¢ã™
                h1 = soup.find('h1')
                if h1 and h1.text.strip():
                    title_candidates.append(h1.text.strip())
                
                # 4. ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                if soup.title and soup.title.string:
                    title_text = soup.title.string.strip()
                    # "iwara"ã®éƒ¨åˆ†ã‚’å‰Šé™¤
                    title_text = re.sub(r'\s*[|\-â€“â€”]\s*iwara.*$', '', title_text, flags=re.IGNORECASE)
                    title_candidates.append(title_text)
                
                # æœ€é©ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
                if title_candidates:
                    page_info['title'] = title_candidates[0]
                
                # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒæŠ½å‡º
                # 1. video-thumbnailã‚¯ãƒ©ã‚¹
                video_thumb = soup.find('img', class_='video-thumbnail')
                if video_thumb and video_thumb.get('src'):
                    page_info['thumbnail'] = urljoin(url, video_thumb.get('src'))
                
                # 2. OGPç”»åƒ
                if not page_info['thumbnail']:
                    og_image = soup.find('meta', property='og:image')
                    if og_image and og_image.get('content'):
                        page_info['thumbnail'] = og_image.get('content')
                
                # 3. ãƒ“ãƒ‡ã‚ªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒ
                if not page_info['thumbnail']:
                    video_preview = soup.find('div', class_='video-preview')
                    if video_preview and video_preview.find('img'):
                        img = video_preview.find('img')
                        if img.get('src'):
                            page_info['thumbnail'] = urljoin(url, img.get('src'))
                
                # èª¬æ˜æŠ½å‡º
                meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', property='og:description')
                if meta_desc and meta_desc.get('content'):
                    page_info['description'] = meta_desc.get('content').strip()
            
            return page_info, raw_html, debug_info
            
        except Exception as e:
            debug_info['error'] = str(e)
            return page_info, None, debug_info
    
    def _process_general_site(self, url, page_info):
        """ä¸€èˆ¬çš„ãªã‚µã‚¤ãƒˆã®å‡¦ç†"""
        debug_info = {}
        
        try:
            # ä¸€èˆ¬çš„ãªãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = self.session.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                raw_html = response.text
                soup = BeautifulSoup(raw_html, 'html.parser')
                
                # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                title_candidates = []
                
                # 1. Open Graph Title
                og_title = soup.find('meta', property='og:title')
                if og_title and og_title.get('content'):
                    title_candidates.append(('og:title', og_title.get('content').strip()))
                
                # 2. Twitter Card Title
                twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
                if twitter_title and twitter_title.get('content'):
                    title_candidates.append(('twitter:title', twitter_title.get('content').strip()))
                
                # 3. HTML Title
                if soup.title and soup.title.string:
                    title_text = soup.title.string.strip()
                    # ã‚µã‚¤ãƒˆåã‚’é™¤å»ã™ã‚‹å‡¦ç†
                    title_text = re.sub(r'\s*[|\-â€“â€”]\s*.*$', '', title_text)
                    title_candidates.append(('html_title', title_text))
                
                # æœ€è‰¯ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
                if title_candidates:
                    page_info['title'] = title_candidates[0][1]
                else:
                    page_info['title'] = f"Saved from {page_info['domain']}"
                
                # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒæŠ½å‡º
                # 1. Open Graph Image
                og_image = soup.find('meta', property='og:image')
                if og_image and og_image.get('content'):
                    page_info['thumbnail'] = og_image.get('content')
                
                # 2. Twitter Card Image
                if not page_info['thumbnail']:
                    twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
                    if twitter_image and twitter_image.get('content'):
                        page_info['thumbnail'] = twitter_image.get('content')
                
                # 3. æœ€åˆã®å¤§ããªç”»åƒ
                if not page_info['thumbnail']:
                    for img in soup.find_all('img', attrs={'width': True, 'height': True}):
                        try:
                            width = int(img.get('width'))
                            height = int(img.get('height'))
                            if width >= 100 and height >= 100:
                                page_info['thumbnail'] = urljoin(url, img.get('src'))
                                break
                        except:
                            pass
                
                # èª¬æ˜æŠ½å‡º
                meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', property='og:description')
                if meta_desc and meta_desc.get('content'):
                    page_info['description'] = meta_desc.get('content').strip()
            
            return page_info, raw_html, debug_info
            
        except Exception as e:
            debug_info['error'] = str(e)
            return page_info, None, debug_info

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.set_page_config(
    page_title="Notion Bookmarker",
    page_icon="ğŸ“š",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ã‚«ã‚¹ã‚¿ãƒ CSSã‚’é©ç”¨
st.markdown("""
<style>
    /* å…¨ä½“ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        max-width: 800px;
    }
    
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ« */
    h1, h2, h3 {
        font-family: 'Helvetica Neue', sans-serif;
        font-weight: 500;
        color: #1E1E1E;
    }
    
    h1 {
        font-size: 2.5rem;
        margin-bottom: 1.5rem;
    }
    
    h2 {
        font-size: 1.8rem;
        margin-top: 2rem;
        margin-bottom: 1rem;
        color: #2E2E2E;
    }
    
    /* ã‚«ãƒ¼ãƒ‰UIã‚¹ã‚¿ã‚¤ãƒ« */
    .css-nahz7x, div.stButton > button, [data-testid="stForm"] {
        border-radius: 12px;
    }
    
    /* å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stTextInput > div > div > input {
        padding: 0.75rem 1rem;
        font-size: 1.1rem;
        border-radius: 8px;
        border: 1px solid #E0E0E0;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    /* ãƒœã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ« */
    div.stButton > button {
        background-color: #4361EE;
        color: white;
        font-weight: 500;
        padding: 0.5rem 1.25rem;
        font-size: 1rem;
        border: none;
        box-shadow: 0 2px 5px rgba(67, 97, 238, 0.3);
        transition: all 0.2s ease;
    }
    
    div.stButton > button:hover {
        background-color: #3A56D4;
        box-shadow: 0 4px 10px rgba(67, 97, 238, 0.4);
        transform: translateY(-1px);
    }
    
    /* æƒ…å ±ã‚«ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .info-card {
        background-color: white;
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        margin: 1.5rem 0;
        border-left: 4px solid #4361EE;
    }
    
    /* ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¸ */
    .domain-badge {
        display: inline-block;
        background-color: #F3F4F6;
        color: #4B5563;
        font-size: 0.85rem;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        margin-top: 0.5rem;
        font-weight: 500;
    }
    
    /* ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .thumbnail-container {
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    
    /* æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .success-message {
        background-color: #10B981;
        color: white;
        padding: 1rem;
        border-radius: 8px;
        display: flex;
        align-items: center;
        margin: 1rem 0;
    }
    
    .success-message svg {
        margin-right: 0.75rem;
    }
    
    /* ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .error-message {
        background-color: #EF4444;
        color: white;
        padding: 1rem;
        border-radius: 8px;
        display: flex;
        align-items: center;
        margin: 1rem 0;
    }
    
    /* ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ– */
    @media (max-width: 768px) {
        .stButton button {
            width: 100%;
            margin-bottom: 0.75rem;
            padding: 0.75rem 1rem;
        }
        
        h1 {
            font-size: 2rem;
        }
        
        .info-card {
            padding: 1.25rem;
        }
    }
    
    /* ãƒ•ãƒƒã‚¿ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ« */
    footer {
        margin-top: 3rem;
        padding-top: 1.5rem;
        border-top: 1px solid #E0E0E0;
        color: #6B7280;
        font-size: 0.9rem;
    }
</style>
""", unsafe_allow_html=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'page_info' not in st.session_state:
    st.session_state['page_info'] = None
if 'loading' not in st.session_state:
    st.session_state['loading'] = False
if 'saving' not in st.session_state:
    st.session_state['saving'] = False
if 'success' not in st.session_state:
    st.session_state['success'] = False
if 'error' not in st.session_state:
    st.session_state['error'] = None
if 'notion_url' not in st.session_state:
    st.session_state['notion_url'] = None
if 'raw_html' not in st.session_state:
    st.session_state['raw_html'] = None
if 'scraper' not in st.session_state:
    st.session_state['scraper'] = AdultSiteScraper()

# ãƒ¡ã‚¤ãƒ³ç”»é¢è¡¨ç¤º - ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
st.markdown("<h1>Notion Bookmarker</h1>", unsafe_allow_html=True)
st.markdown("""
<div style="margin-bottom: 2rem;">
    <p style="font-size: 1.1rem; color: #4B5563; margin-bottom: 1.5rem;">
        ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ç°¡å˜ã«Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    </p>
</div>
""", unsafe_allow_html=True)

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
def get_metadata_advanced(url):
    """Webãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæˆäººå‘ã‘ã‚µã‚¤ãƒˆå¯¾å¿œï¼‰"""
    scraper = st.session_state['scraper']
    return scraper.get_page_info(url)

# Notionã«æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°
def add_to_notion(page_info):
    try:
        # Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        notion = Client(auth=NOTION_API_TOKEN)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        db = notion.databases.retrieve(database_id=DATABASE_ID)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ç¢ºèª
        properties = {}
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (å¿…é ˆ) ã‚’æ¤œå‡º
        title_field = None
        for name, prop in db['properties'].items():
            if prop['type'] == 'title':
                title_field = name
                properties[name] = {
                    'title': [{'text': {'content': page_info['title']}}]
                }
                break
        
        if not title_field:
            return False, "ã‚¿ã‚¤ãƒˆãƒ«ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        # URLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'URL' in db['properties'] and db['properties']['URL']['type'] == 'url':
            properties['URL'] = {'url': page_info['url']}
        
        # ã‚¿ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ã‚¿ã‚°' in db['properties'] and db['properties']['ã‚¿ã‚°']['type'] == 'multi_select':
            properties['ã‚¿ã‚°'] = {'multi_select': []}
        
        # ä½œæˆæ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ä½œæˆæ—¥æ™‚' in db['properties'] and db['properties']['ä½œæˆæ—¥æ™‚']['type'] == 'date':
            properties['ä½œæˆæ—¥æ™‚'] = {
                'date': {
                    'start': datetime.now().isoformat()
                }
            }
        
        # Notionãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
        new_page = notion.pages.create(
            parent={'database_id': DATABASE_ID},
            properties=properties
        )
        
        # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€å­ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ 
        if page_info['thumbnail']:
            try:
                notion.blocks.children.append(
                    block_id=new_page['id'],
                    children=[
                        {
                            "object": "block",
                            "type": "image",
                            "image": {
                                "type": "external",
                                "external": {
                                    "url": page_info['thumbnail']
                                }
                            }
                        }
                    ]
                )
            except Exception:
                pass
        
        return True, new_page['url']
    
    except Exception as e:
        return False, str(e)

# URLå…¥åŠ›ã‚¨ãƒªã‚¢ - ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥ãªãƒ‡ã‚¶ã‚¤ãƒ³
st.markdown("""
<div style="background-color: white; padding: 1.75rem; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.08); margin-bottom: 2rem;">
    <h3 style="margin-top: 0; margin-bottom: 1rem; font-size: 1.3rem; color: #333;">ğŸ“Œ ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›</h3>
</div>
""", unsafe_allow_html=True)

# URLã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
url = st.text_input("", placeholder="https://example.com", label_visibility="collapsed")

# æ¤œç´¢ãƒœã‚¿ãƒ³ã¨ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã®ç®¡ç†
col1, col2 = st.columns([1, 3])
with col1:
    fetch_button = st.button("æƒ…å ±ã‚’å–å¾—", key="fetch_button", use_container_width=True)
with col2:
    pass

if fetch_button and url:
    st.session_state['loading'] = True
    st.session_state['page_info'] = None
    st.session_state['success'] = False
    st.session_state['error'] = None
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = st.progress(0)
    
    # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    with st.spinner("ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—ä¸­..."):
        for percent_complete in range(0, 80, 10):
            time.sleep(0.1)
            progress_bar.progress(percent_complete)
            
        page_info, raw_html, debug_info = get_metadata_advanced(url)
        
        for percent_complete in range(80, 101, 5):
            time.sleep(0.05)
            progress_bar.progress(percent_complete)
    
    st.session_state['page_info'] = page_info
    st.session_state['raw_html'] = raw_html
    st.session_state['debug_info'] = debug_info
    st.session_state['loading'] = False
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®Œäº†çŠ¶æ…‹ã«ã—ã¦å°‘ã—å¾…ã£ã¦ã‹ã‚‰æ¶ˆã™
    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()
    
    # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º
    st.rerun()

# æƒ…å ±ã‚’å–å¾—ã—ãŸå¾Œã®è¡¨ç¤º
if st.session_state['page_info']:
    page_info = st.session_state['page_info']
    
    # æŠ½å‡ºã—ãŸæƒ…å ±ã‚’ã‚«ãƒ¼ãƒ‰UIã§è¡¨ç¤º
    st.markdown("""
    <h2>å–å¾—ã—ãŸæƒ…å ±</h2>
    """, unsafe_allow_html=True)
    
    st.markdown(f"""
    <div class="info-card">
        <h3 style="margin-top: 0; margin-bottom: 0.75rem; font-size: 1.4rem;">{page_info['title']}</h3>
        <a href="{page_info['url']}" target="_blank" style="color: #4361EE; text-decoration: none; font-size: 1rem; display: block; margin-bottom: 0.75rem;">{page_info['url']}</a>
        <div class="domain-badge">{page_info['domain']}</div>
    </div>
    """, unsafe_allow_html=True)
    
    # ã‚µãƒ ãƒã‚¤ãƒ« - use_container_widthã‚’ä½¿ç”¨
    if page_info['thumbnail']:
        st.image(page_info['thumbnail'], caption="ã‚µãƒ ãƒã‚¤ãƒ«", use_container_width=True)
    else:
        st.warning("ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # æ‰‹å‹•ã§ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’å…¥åŠ›ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        manual_thumbnail = st.text_input("ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’æ‰‹å‹•ã§å…¥åŠ›:", placeholder="https://example.com/image.jpg")
        if manual_thumbnail:
            try:
                st.image(manual_thumbnail, caption="å…¥åŠ›ã•ã‚ŒãŸã‚µãƒ ãƒã‚¤ãƒ«", use_container_width=True)
                page_info['thumbnail'] = manual_thumbnail
                st.session_state['page_info'] = page_info
                st.success("ã‚µãƒ ãƒã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
    
    # èª¬æ˜
    if page_info.get('description'):
        st.markdown("**èª¬æ˜**:")
        st.write(page_info['description'])
    
    # ã‚¿ã‚¤ãƒˆãƒ«æ‰‹å‹•ç·¨é›†æ©Ÿèƒ½
    st.subheader("ã‚¿ã‚¤ãƒˆãƒ«ã®ç·¨é›†")
    edited_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç·¨é›†:", value=page_info['title'])
    if edited_title != page_info['title']:
        page_info['title'] = edited_title
        st.session_state['page_info'] = page_info
        st.success("ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    
    # ä¿å­˜ãƒœã‚¿ãƒ³ - ã‚¢ã‚¯ã‚»ãƒ³ãƒˆã‚«ãƒ©ãƒ¼ä½¿ç”¨
    save_col1, save_col2 = st.columns([1, 3])
    with save_col1:
        save_button = st.button("Notionã«ä¿å­˜", key="save_button", use_container_width=True)
    with save_col2:
        pass
    
    # ä¿å­˜ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã¨ã
    if save_button:
        st.session_state['saving'] = True
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
        save_progress = st.progress(0)
        for percent_complete in range(0, 101, 20):
            time.sleep(0.1)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®é…å»¶
            save_progress.progress(percent_complete)
        
        # Notionã«ä¿å­˜
        success, result = add_to_notion(page_info)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®Œäº†çŠ¶æ…‹ã«ã—ã¦å°‘ã—å¾…ã£ã¦ã‹ã‚‰æ¶ˆã™
        save_progress.progress(100)
        time.sleep(0.5)
        save_progress.empty()
        
        st.session_state['saving'] = False
        st.session_state['success'] = success
        
        if success:
            st.session_state['notion_url'] = result
        else:
            st.session_state['error'] = result
        
        # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦çµæœã‚’è¡¨ç¤º
        st.rerun()
    
    # ä¿å­˜æˆåŠŸæ™‚ã®è¡¨ç¤º
    if st.session_state['success']:
        st.success("âœ… Notionã«ä¿å­˜ã—ã¾ã—ãŸï¼")
        
        if st.session_state.get('notion_url'):
            st.markdown(f"[Notionã§é–‹ã]({st.session_state['notion_url']})")
    
    # ã‚¨ãƒ©ãƒ¼æ™‚ã®è¡¨ç¤º
    if st.session_state['error']:
        st.error(f"âŒ ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {st.session_state['error']}")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("""
<footer>
    <p>Â© 2025 Notion Bookmarker - ã™ã¹ã¦ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’Notionã«ä¿å­˜</p>
</footer>
""", unsafe_allow_html=True)
import streamlit as st
import requests
from bs4 import BeautifulSoup
from notion_client import Client
from urllib.parse import urlparse, urljoin
from datetime import datetime
import time
import re
import json
import logging

# å›ºå®šã®èªè¨¼æƒ…å ±
NOTION_API_TOKEN = "ntn_i2957150244j9hSJCmlhWx1tkxlBP2MNliQk9Z3AkBHgcK"
DATABASE_ID = "1b90b0428824814fa0d9db921aa812d0"

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("NotionBookmarker")

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.set_page_config(
    page_title="Notion Bookmarker",
    page_icon="ğŸ“š",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ã‚«ã‚¹ã‚¿ãƒ CSSã‚’é©ç”¨
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        margin-bottom: 2rem;
        color: #4361EE;
    }
    .info-card {
        padding: 1.5rem;
        border-radius: 0.5rem;
        background-color: #f8f9fa;
        margin-bottom: 1.5rem;
        border-left: 4px solid #4361EE;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .domain-badge {
        display: inline-block;
        background-color: #e9ecef;
        border-radius: 2rem;
        padding: 0.25rem 0.75rem;
        font-size: 0.75rem;
        margin-right: 0.5rem;
        margin-bottom: 0.5rem;
    }
    .content-type-badge {
        display: inline-block;
        background-color: #4361EE;
        color: white;
        border-radius: 2rem;
        padding: 0.25rem 0.75rem;
        font-size: 0.75rem;
        margin-right: 0.5rem;
        margin-bottom: 0.5rem;
        font-weight: 600;
    }
    .btn-primary {
        background-color: #4361EE;
        color: white;
    }
    .section-header {
        margin-top: 1.5rem;
        margin-bottom: 1rem;
        color: #343a40;
        border-bottom: 2px solid #4361EE;
        padding-bottom: 0.5rem;
    }
    footer {
        margin-top: 3rem;
        text-align: center;
        color: #6c757d;
        font-size: 0.8rem;
    }
</style>
""", unsafe_allow_html=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
def init_session_state():
    if 'page_info' not in st.session_state:
        st.session_state['page_info'] = None
    if 'loading' not in st.session_state:
        st.session_state['loading'] = False
    if 'saving' not in st.session_state:
        st.session_state['saving'] = False
    if 'success' not in st.session_state:
        st.session_state['success'] = False
    if 'error' not in st.session_state:
        st.session_state['error'] = None
    if 'notion_url' not in st.session_state:
        st.session_state['notion_url'] = None
    if 'raw_html' not in st.session_state:
        st.session_state['raw_html'] = None

init_session_state()

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0',
    'Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.104 Mobile Safari/537.36'
]

def guess_content_type(url, soup):
    """URLã¨HTMLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬ã™ã‚‹"""
    domain = urlparse(url).netloc
    url_lower = url.lower()
    
    # URLã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚§ãƒƒã‚¯
    if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif']):
        return 'image'
    elif any(ext in url_lower for ext in ['.mp4', '.avi', '.mov', '.wmv']):
        return 'video'
    elif any(ext in url_lower for ext in ['.pdf', '.doc', '.docx', '.ppt', '.xls']):
        return 'document'
    
    # ã‚¢ãƒ‹ãƒ¡é–¢é€£ã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['crunchyroll', 'funimation', 'animelab', 'myanimelist', 'anilist']):
        return 'anime'
    
    # æ¼«ç”»é–¢é€£ã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['mangadex', 'mangaplus', 'comixology', 'manga-up']):
        return 'manga'
    
    # ASMRé–¢é€£ã‚µã‚¤ãƒˆ
    if any(keyword in domain or keyword in url_lower for keyword in ['asmr', 'whispering', 'binaural']):
        return 'ASMR'
    
    # ä¸€èˆ¬çš„ãªå‹•ç”»ã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['youtube.com', 'youtu.be', 'vimeo.com', 'nicovideo.jp']):
        return 'video'
    
    # ä¸€èˆ¬çš„ãªç”»åƒã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['instagram.com', 'flickr.com', 'imgur.com', 'pixiv.net']):
        return 'image'
    
    # SNSã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['twitter.com', 'facebook.com', 'linkedin.com']):
        return 'social'
    
    # ECã‚µã‚¤ãƒˆ
    if any(site in domain for site in ['amazon', 'rakuten', 'shopping.yahoo']):
        return 'product'
    
    # HTMLã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§åˆ¤å®š
    if soup:
        # Open Graphã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
        og_type = soup.find('meta', property='og:type')
        if og_type and og_type.get('content'):
            og_content = og_type.get('content').lower()
            if 'video' in og_content:
                return 'video'
            elif 'article' in og_content:
                return 'article'
            elif 'product' in og_content:
                return 'product'
            elif 'music' in og_content:
                return 'music'
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_meta and keywords_meta.get('content'):
            keywords = keywords_meta.get('content').lower()
            if 'anime' in keywords:
                return 'anime'
            elif 'manga' in keywords:
                return 'manga'
            elif 'asmr' in keywords:
                return 'ASMR'
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        page_text = ''
        if soup.title:
            page_text += soup.title.string.lower() if soup.title.string else ''
        
        if 'anime' in page_text:
            return 'anime'
        elif 'manga' in page_text:
            return 'manga'
        elif 'asmr' in page_text:
            return 'ASMR'
        
        # ãƒ“ãƒ‡ã‚ªè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
        if soup.find('video') or soup.find('iframe', src=lambda x: x and ('youtube.com' in x or 'vimeo.com' in x)):
            return 'video'
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨˜äº‹
    return 'article'

def get_metadata_advanced(url):
    """é«˜åº¦ãªæ–¹æ³•ã§Webãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    
    # åˆæœŸåŒ–
    page_info = {
        'title': None,
        'description': None,
        'thumbnail': None,
        'url': url,
        'domain': urlparse(url).netloc,
        'content_type': None
    }
    raw_html = None
    best_html = None
    debug_info = {}
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦å†åˆ©ç”¨ã™ã‚‹
    session = requests.Session()
    
    # è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ˜ãƒƒãƒ€ãƒ¼ã§è©¦ã™
    for idx, agent in enumerate(USER_AGENTS):
        try:
            headers = {
                'User-Agent': agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Cache-Control': 'no-cache',
                'Referer': 'https://www.google.com/',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'dnt': '1'
            }
            
            debug_info[f'request_{idx}'] = {
                'user_agent': agent,
                'headers': headers
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = session.get(url, headers=headers, timeout=20, allow_redirects=True)
            debug_info[f'response_{idx}'] = {
                'status_code': response.status_code,
                'content_type': response.headers.get('Content-Type', 'unknown'),
                'encoding': response.encoding
            }
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ200ä»¥å¤–ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if response.status_code != 200:
                continue
            
            # HTMLã®è§£æ
            html_content = response.text
            
            # æœ€åˆã®æˆåŠŸã—ãŸHTMLã‚’ä¿å­˜
            if raw_html is None:
                raw_html = html_content
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„HTMLã‚’æ¤œç´¢
            if 'title' in html_content.lower() or 'og:title' in html_content.lower():
                best_html = html_content
                break
        
        except Exception as e:
            debug_info[f'error_{idx}'] = str(e)
            continue
    
    # æœ€è‰¯ã®HTMLã‚’ä½¿ç”¨ï¼ˆãªã‘ã‚Œã°æœ€åˆã®HTMLï¼‰
    html_to_parse = best_html or raw_html
    
    # HTMLè§£æ
    if html_to_parse:
        try:
            soup = BeautifulSoup(html_to_parse, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_candidates = []
            
            # 1. Open Graph Title
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                title_candidates.append(('og:title', og_title.get('content').strip()))
            
            # 2. Twitter Card Title
            twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
            if twitter_title and twitter_title.get('content'):
                title_candidates.append(('twitter:title', twitter_title.get('content').strip()))
            
            # 3. HTML Title
            if soup.title and soup.title.string:
                title_text = soup.title.string.strip()
                # ã‚µã‚¤ãƒˆåã‚’é™¤å»ã™ã‚‹å‡¦ç†
                title_text = re.sub(r'\s*[|\-â€“â€”]\s*.*$', '', title_text)
                title_candidates.append(('html_title', title_text))
            
            # 4. H1 Tag
            h1 = soup.find('h1')
            if h1 and h1.text.strip():
                title_candidates.append(('h1', h1.text.strip()))
            
            # æœ€è‰¯ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
            if title_candidates:
                # ã‚¿ã‚¤ãƒˆãƒ«å€™è£œã‚’è¨˜éŒ²
                debug_info['title_candidates'] = title_candidates
                
                # æœ€åˆã®å€™è£œã‚’ä½¿ç”¨ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
                page_info['title'] = title_candidates[0][1]
            else:
                page_info['title'] = f"Saved from {page_info['domain']}"
            
            # èª¬æ˜æ–‡æŠ½å‡º
            description_candidates = []
            
            # 1. Open Graph Description
            og_desc = soup.find('meta', property='og:description')
            if og_desc and og_desc.get('content'):
                description_candidates.append(('og:description', og_desc.get('content').strip()))
            
            # 2. Twitter Card Description
            twitter_desc = soup.find('meta', attrs={'name': 'twitter:description'})
            if twitter_desc and twitter_desc.get('content'):
                description_candidates.append(('twitter:description', twitter_desc.get('content').strip()))
            
            # 3. Meta Description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                description_candidates.append(('meta_description', meta_desc.get('content').strip()))
            
            # æœ€è‰¯ã®èª¬æ˜æ–‡ã‚’é¸æŠ
            if description_candidates:
                page_info['description'] = description_candidates[0][1]
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒæŠ½å‡º
            image_candidates = []
            
            # 1. Open Graph Image
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                image_url = og_image.get('content').strip()
                if not image_url.startswith(('http://', 'https://')):
                    image_url = urljoin(url, image_url)
                image_candidates.append(('og:image', image_url))
            
            # 2. Twitter Card Image
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                image_url = twitter_image.get('content').strip()
                if not image_url.startswith(('http://', 'https://')):
                    image_url = urljoin(url, image_url)
                image_candidates.append(('twitter:image', image_url))
                
            # 3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®ç”»åƒã‚’æ¢ã™
            content_classes = ['entry-content', 'article-content', 'content', 'post-content']
            for class_name in content_classes:
                content_area = soup.find(class_=class_name)
                if content_area:
                    images = content_area.find_all('img', src=True)
                    for img in images:
                        image_url = img['src']
                        if not image_url.startswith(('http://', 'https://')):
                            image_url = urljoin(url, image_url)
                        image_candidates.append(('content_image', image_url))
                        break  # æœ€åˆã®1ã¤ã ã‘å–å¾—
            
            # 4. ã‚®ãƒ£ãƒ©ãƒªãƒ¼å†…ã®ç”»åƒã‚’æ¢ã™
            gallery_classes = ['gallery', 'fotorama', 'carousel', 'slider']
            for class_name in gallery_classes:
                gallery = soup.find(class_=class_name)
                if gallery:
                    images = gallery.find_all('img', src=True)
                    for img in images:
                        image_url = img['src']
                        if not image_url.startswith(('http://', 'https://')):
                            image_url = urljoin(url, image_url)
                        image_candidates.append(('gallery_image', image_url))
                        break  # æœ€åˆã®1ã¤ã ã‘å–å¾—
            
            # æœ€è‰¯ã®ç”»åƒã‚’é¸æŠ
            if image_candidates:
                page_info['thumbnail'] = image_candidates[0][1]
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬
            page_info['content_type'] = guess_content_type(url, soup)
            
        except Exception as e:
            debug_info['parsing_error'] = str(e)
    
    # ã‚¿ã‚¤ãƒˆãƒ«ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not page_info['title'] or page_info['title'] == f"Saved from {page_info['domain']}":
        # MetaScraperã‚µãƒ¼ãƒ“ã‚¹ã‚’è©¦ã™
        try:
            meta_response = requests.get(f"https://api.microlink.io/?url={url}")
            if meta_response.status_code == 200:
                meta_data = meta_response.json()
                if meta_data.get('status') == 'success':
                    data = meta_data.get('data', {})
                    if data.get('title'):
                        page_info['title'] = data['title']
                    if data.get('description') and not page_info['description']:
                        page_info['description'] = data['description']
                    if data.get('image') and data['image'].get('url') and not page_info['thumbnail']:
                        page_info['thumbnail'] = data['image']['url']
        except:
            pass
    
    # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not page_info['title'] or page_info['title'] == f"Saved from {page_info['domain']}":
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰ã®ç”Ÿæˆ
        domain_parts = page_info['domain'].split('.')
        if len(domain_parts) > 1:
            page_info['title'] = f"Content from {domain_parts[-2].capitalize()}"
    
    # çµæœã‚’è¿”ã™
    return page_info, raw_html, debug_info

def add_to_notion(page_info):
    """Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹"""
    try:
        # Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        notion = Client(auth=NOTION_API_TOKEN)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        db = notion.databases.retrieve(database_id=DATABASE_ID)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ç¢ºèª
        properties = {}
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (å¿…é ˆ) ã‚’æ¤œå‡º
        title_field = None
        for name, prop in db['properties'].items():
            if prop['type'] == 'title':
                title_field = name
                properties[name] = {
                    'title': [{'text': {'content': page_info['title']}}]
                }
                break
        
        if not title_field:
            return False, "ã‚¿ã‚¤ãƒˆãƒ«ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        # URLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'URL' in db['properties'] and db['properties']['URL']['type'] == 'url':
            properties['URL'] = {'url': page_info['url']}

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ã‚«ãƒ†ã‚´ãƒª' in db['properties'] and db['properties']['ã‚«ãƒ†ã‚´ãƒª']['type'] == 'select':
            content_type = page_info.get('content_type', 'article')
            properties['ã‚«ãƒ†ã‚´ãƒª'] = {'select': {'name': content_type}}
                
        # ã‚¿ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ - ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚¿ã‚°ã¨ã—ã¦è¿½åŠ ã€Œã—ãªã„ã€
        if 'ã‚¿ã‚°' in db['properties'] and db['properties']['ã‚¿ã‚°']['type'] == 'multi_select':
            # ã“ã“ã§ã‚¿ã‚°ã‚’ç©ºã®é…åˆ—ã¨ã—ã¦è¨­å®šï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è¿½åŠ ã—ãªã„ï¼‰
            properties['ã‚¿ã‚°'] = {'multi_select': []}
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'ã‚½ãƒ¼ã‚¹' in db['properties'] and db['properties']['ã‚½ãƒ¼ã‚¹']['type'] == 'select':
            properties['ã‚½ãƒ¼ã‚¹'] = {'select': {'name': page_info['domain']}}
            
        # èª¬æ˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if 'èª¬æ˜' in db['properties'] and db['properties']['èª¬æ˜']['type'] in ['rich_text', 'text']:
            if page_info.get('description'):
                properties['èª¬æ˜'] = {
                    'rich_text': [{'text': {'content': page_info['description'][:2000]}}]
                }
        
        # Notionãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
        new_page = notion.pages.create(
            parent={'database_id': DATABASE_ID},
            properties=properties
        )
        
        # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€å­ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ 
        if page_info.get('thumbnail'):
            try:
                notion.blocks.children.append(
                    block_id=new_page['id'],
                    children=[
                        {
                            "object": "block",
                            "type": "image",
                            "image": {
                                "type": "external",
                                "external": {
                                    "url": page_info['thumbnail']
                                }
                            }
                        }
                    ]
                )
            except Exception as e:
                logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®è¿½åŠ ã«å¤±æ•—: {str(e)}")
        
        return True, new_page['url']
    
    except Exception as e:
        logger.error(f"Notionè¿½åŠ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False, str(e)

# ãƒ¡ã‚¤ãƒ³ç”»é¢
st.markdown("<h1 class='main-header'>Notion Bookmarker</h1>", unsafe_allow_html=True)

# URLå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
url = st.text_input("ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã™ã‚‹URLã‚’å…¥åŠ›", placeholder="https://example.com")

# æ¤œç´¢ãƒœã‚¿ãƒ³ã¨ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã®ç®¡ç†
col1, col2 = st.columns([1, 3])
with col1:
    fetch_button = st.button("æƒ…å ±ã‚’å–å¾—", key="fetch_button", use_container_width=True)

if fetch_button and url:
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    st.session_state['loading'] = True
    st.session_state['page_info'] = None
    st.session_state['success'] = False
    st.session_state['error'] = None
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = st.progress(0)
    for percent_complete in range(0, 101, 10):
        time.sleep(0.05)
        progress_bar.progress(percent_complete)
    
    # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’æŠ½å‡º
    try:
        page_info, raw_html, debug_info = get_metadata_advanced(url)
        st.session_state['page_info'] = page_info
        st.session_state['raw_html'] = raw_html
    except Exception as e:
        st.session_state['error'] = f"æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    st.session_state['loading'] = False
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®Œäº†
    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()
    
    # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
    st.rerun()

# æƒ…å ±ã‚’å–å¾—ã—ãŸå¾Œã®è¡¨ç¤º
if st.session_state['page_info']:
    page_info = st.session_state['page_info']
    
    # æŠ½å‡ºã—ãŸæƒ…å ±ã‚’ã‚«ãƒ¼ãƒ‰UIã§è¡¨ç¤º
    st.markdown("<h2 class='section-header'>å–å¾—ã—ãŸæƒ…å ±</h2>", unsafe_allow_html=True)
    
    st.markdown(f"""
    <div class="info-card">
        <h3 style="margin-top: 0; margin-bottom: 0.75rem; font-size: 1.4rem;">{page_info['title']}</h3>
        <a href="{page_info['url']}" target="_blank" style="color: #4361EE; text-decoration: none; font-size: 1rem; display: block; margin-bottom: 0.75rem;">{page_info['url']}</a>
        <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
            <div class="domain-badge">{page_info['domain']}</div>
            <div class="content-type-badge">{page_info.get('content_type', 'article')}</div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # ã‚µãƒ ãƒã‚¤ãƒ«è¡¨ç¤º
    if page_info.get('thumbnail'):
        st.image(page_info['thumbnail'], caption="ã‚µãƒ ãƒã‚¤ãƒ«", use_container_width=True)
    
    # èª¬æ˜
    if page_info.get('description'):
        st.markdown("**èª¬æ˜**:")
        st.write(page_info['description'])
    
    # ç·¨é›†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("<h2 class='section-header'>æƒ…å ±ã®ç·¨é›†</h2>", unsafe_allow_html=True)
    
    # ã‚¿ã‚¤ãƒˆãƒ«ç·¨é›†
    edited_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç·¨é›†:", value=page_info['title'])
    if edited_title != page_info['title']:
        page_info['title'] = edited_title
        st.session_state['page_info'] = page_info
        st.success("ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ")

    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ç·¨é›†
    content_types = ['article', 'video', 'image', 'social', 'product', 'document', 'music', 'anime', 'manga', 'ASMR', 'other']
    selected_type = st.selectbox(
        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—:", 
        options=content_types, 
        index=content_types.index(page_info.get('content_type', 'article')) if page_info.get('content_type') in content_types else 0
    )
    if selected_type != page_info.get('content_type'):
        page_info['content_type'] = selected_type
        st.session_state['page_info'] = page_info
        st.success("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    
    # ä¿å­˜ãƒœã‚¿ãƒ³
    save_col1, save_col2 = st.columns([1, 3])
    with save_col1:
        save_button = st.button("Notionã«ä¿å­˜", key="save_button", use_container_width=True)
    
    if save_button:
        st.session_state['saving'] = True
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        save_progress = st.progress(0)
        for percent_complete in range(0, 101, 20):
            time.sleep(0.1)
            save_progress.progress(percent_complete)
        
        # Notionã«ä¿å­˜
        success, result = add_to_notion(page_info)
        
        save_progress.progress(100)
        time.sleep(0.5)
        save_progress.empty()
        
        st.session_state['saving'] = False
        
        if success:
            st.session_state['success'] = True
            st.session_state['notion_url'] = result
            st.success("âœ… Notionã«ä¿å­˜ã—ã¾ã—ãŸï¼")
            st.markdown(f"[Notionã§é–‹ã]({result})")
        else:
            st.session_state['error'] = result
            st.error(f"âŒ ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {result}")

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
if st.session_state.get('error'):
    st.error(st.session_state['error'])

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("""
<footer>
    <p>Â© 2025 Notion Bookmarker</p>
</footer>
""", unsafe_allow_html=True)